{"nbformat":4,"nbformat_minor":0,"metadata":{"interpreter":{"hash":"bee0532d505904fbb5a3281d4f74ffd15ae670e264452312d08c7f434d534dd4"},"kernelspec":{"display_name":"Python 3.8.8 64-bit","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.8"},"orig_nbformat":4,"colab":{"name":"Blood_smear_analyser.ipynb","provenance":[]},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"46f750f655f64914bab6f9e5ab923122":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_b8be9def5a1d44e4a5b58e85681fbf78","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_93834be7094f46a8aa6c638480e32bd3","IPY_MODEL_99c27c2fc84e49b99d5a003f1f8b05d1","IPY_MODEL_eddb7798a9f54323bd4ab7226741a491"]}},"b8be9def5a1d44e4a5b58e85681fbf78":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"93834be7094f46a8aa6c638480e32bd3":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_1f33fd3dc81f4b9a863fd87d91f3eafd","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_4228fd514afe46479c9d0ce49fd53a3a"}},"99c27c2fc84e49b99d5a003f1f8b05d1":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_9843448a51054e2aaf0e459beefddeb5","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":32342954,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":32342954,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_939194f261604b33843e4884636d31c8"}},"eddb7798a9f54323bd4ab7226741a491":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_f046321876224771bdd39ee872c02659","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 30.8M/30.8M [00:01&lt;00:00, 15.2MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_1bb3b40226e34fb9a5813fe03dcbc967"}},"1f33fd3dc81f4b9a863fd87d91f3eafd":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"4228fd514afe46479c9d0ce49fd53a3a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"9843448a51054e2aaf0e459beefddeb5":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"939194f261604b33843e4884636d31c8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"f046321876224771bdd39ee872c02659":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"1bb3b40226e34fb9a5813fe03dcbc967":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"code","metadata":{"id":"OIk8Sd8HUsY7","executionInfo":{"status":"ok","timestamp":1634525904937,"user_tz":-60,"elapsed":25337,"user":{"displayName":"Mitterrand Ekole","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiX0MYHQxcncfbSTaJ_vKfXwMbcZK_EEhm_DqDAtw=s64","userId":"01893583269197876978"}}},"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.optim import lr_scheduler\n","from torchvision import datasets, models, transforms\n","import time\n","import os\n","import copy\n","from collections import OrderedDict\n","\n","from torch.autograd import variable\n","\n","import shutil"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uG-3hYudUzkx","executionInfo":{"status":"ok","timestamp":1634525879604,"user_tz":-60,"elapsed":29851,"user":{"displayName":"Mitterrand Ekole","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiX0MYHQxcncfbSTaJ_vKfXwMbcZK_EEhm_DqDAtw=s64","userId":"01893583269197876978"}},"outputId":"081e90fb-ea00-4876-b923-06b7fc9f6bd0"},"source":["from google.colab import drive\n","\n","drive.mount('/content/drive')"],"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0-xONLpKUsZE","executionInfo":{"status":"ok","timestamp":1634525912587,"user_tz":-60,"elapsed":340,"user":{"displayName":"Mitterrand Ekole","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiX0MYHQxcncfbSTaJ_vKfXwMbcZK_EEhm_DqDAtw=s64","userId":"01893583269197876978"}},"outputId":"1847420b-b2ed-43f5-d78e-f4fe1ee9d24e"},"source":["num_classes = 2 \n","# check if CUDA is available\n","train_on_gpu = torch.cuda.is_available()\n","\n","if not train_on_gpu:\n","    print('CUDA is not available.  Training on CPU ...')\n","else:\n","    print('CUDA is available!  Training on GPU ...')"],"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["CUDA is available!  Training on GPU ...\n"]}]},{"cell_type":"code","metadata":{"id":"sXNVvyu5UsZH","executionInfo":{"status":"ok","timestamp":1634525914766,"user_tz":-60,"elapsed":370,"user":{"displayName":"Mitterrand Ekole","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiX0MYHQxcncfbSTaJ_vKfXwMbcZK_EEhm_DqDAtw=s64","userId":"01893583269197876978"}}},"source":["\n","data_dir = '/content/drive/MyDrive/Colab Notebooks/Malaria Detector Code/cell_images'\n","train_dir = data_dir + '/Train'\n","test_dir = data_dir + '/Test'\n"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"xdEfZTPOUsZJ","executionInfo":{"status":"ok","timestamp":1634525917346,"user_tz":-60,"elapsed":498,"user":{"displayName":"Mitterrand Ekole","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiX0MYHQxcncfbSTaJ_vKfXwMbcZK_EEhm_DqDAtw=s64","userId":"01893583269197876978"}}},"source":["data_transforms= transforms.Compose(\n","    [\n","        transforms.Resize((224,224)),\n","        transforms.ToTensor(),\n","        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n","    ]\n",")\n"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"p2x3Z_IJUsZK","executionInfo":{"status":"ok","timestamp":1634525932721,"user_tz":-60,"elapsed":2676,"user":{"displayName":"Mitterrand Ekole","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiX0MYHQxcncfbSTaJ_vKfXwMbcZK_EEhm_DqDAtw=s64","userId":"01893583269197876978"}}},"source":["\n","    \n","train_data = datasets.ImageFolder(train_dir, transform=data_transforms)\n","test_data = datasets.ImageFolder(test_dir, transform=data_transforms)\n"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"etOsMauIUsZL","executionInfo":{"status":"ok","timestamp":1634525933778,"user_tz":-60,"elapsed":3,"user":{"displayName":"Mitterrand Ekole","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiX0MYHQxcncfbSTaJ_vKfXwMbcZK_EEhm_DqDAtw=s64","userId":"01893583269197876978"}}},"source":["batch_size = 64\n","num_workers=0\n","\n","# TODO: Using the image datasets and the trainforms, define the dataloaders\n","train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, \n","                                           num_workers=num_workers, shuffle=True)\n","test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, \n","                                          num_workers=num_workers, shuffle=True)\n"],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":66,"referenced_widgets":["46f750f655f64914bab6f9e5ab923122","b8be9def5a1d44e4a5b58e85681fbf78","93834be7094f46a8aa6c638480e32bd3","99c27c2fc84e49b99d5a003f1f8b05d1","eddb7798a9f54323bd4ab7226741a491","1f33fd3dc81f4b9a863fd87d91f3eafd","4228fd514afe46479c9d0ce49fd53a3a","9843448a51054e2aaf0e459beefddeb5","939194f261604b33843e4884636d31c8","f046321876224771bdd39ee872c02659","1bb3b40226e34fb9a5813fe03dcbc967"]},"id":"1WW3UMWRUsZM","executionInfo":{"status":"ok","timestamp":1634525939014,"user_tz":-60,"elapsed":2623,"user":{"displayName":"Mitterrand Ekole","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiX0MYHQxcncfbSTaJ_vKfXwMbcZK_EEhm_DqDAtw=s64","userId":"01893583269197876978"}},"outputId":"604df4d1-6aeb-47a3-ae0a-7426d1e93df6"},"source":["#defining the pretrained network\n","\n","neural_net=models.densenet121(pretrained=True)"],"execution_count":9,"outputs":[{"output_type":"stream","name":"stderr","text":["Downloading: \"https://download.pytorch.org/models/densenet121-a639ec97.pth\" to /root/.cache/torch/hub/checkpoints/densenet121-a639ec97.pth\n"]},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"46f750f655f64914bab6f9e5ab923122","version_minor":0,"version_major":2},"text/plain":["  0%|          | 0.00/30.8M [00:00<?, ?B/s]"]},"metadata":{}}]},{"cell_type":"code","metadata":{"id":"-BPNCb2xUsZO","executionInfo":{"status":"ok","timestamp":1634525943269,"user_tz":-60,"elapsed":341,"user":{"displayName":"Mitterrand Ekole","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiX0MYHQxcncfbSTaJ_vKfXwMbcZK_EEhm_DqDAtw=s64","userId":"01893583269197876978"}}},"source":["#freeze feature paramters\n","\n","for params in neural_net.parameters():\n","    params.requires_grad=True"],"execution_count":10,"outputs":[]},{"cell_type":"code","metadata":{"id":"QsxNX3y1UsZP","executionInfo":{"status":"ok","timestamp":1634525956589,"user_tz":-60,"elapsed":3,"user":{"displayName":"Mitterrand Ekole","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiX0MYHQxcncfbSTaJ_vKfXwMbcZK_EEhm_DqDAtw=s64","userId":"01893583269197876978"}}},"source":["classifier=nn.Sequential(OrderedDict([\n","    ('fc1', nn.Linear(1024, 200)),\n","    ('relu', nn.ReLU()), \n","    ('fc2', nn.Linear(200, 102)),\n","    ('drop', nn.Dropout(p=0.5)),\n","    ('output', nn.LogSoftmax(dim=1))\n","]))"],"execution_count":11,"outputs":[]},{"cell_type":"code","metadata":{"id":"LfhP36BpUsZP","executionInfo":{"status":"ok","timestamp":1634525957057,"user_tz":-60,"elapsed":4,"user":{"displayName":"Mitterrand Ekole","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiX0MYHQxcncfbSTaJ_vKfXwMbcZK_EEhm_DqDAtw=s64","userId":"01893583269197876978"}}},"source":["neural_net.classfier=classifier\n"],"execution_count":12,"outputs":[]},{"cell_type":"code","metadata":{"id":"rkWbS77udt6U","executionInfo":{"status":"ok","timestamp":1634525962021,"user_tz":-60,"elapsed":340,"user":{"displayName":"Mitterrand Ekole","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiX0MYHQxcncfbSTaJ_vKfXwMbcZK_EEhm_DqDAtw=s64","userId":"01893583269197876978"}}},"source":["def model_save(state, is_best=False, filename='savepoint.pth'):\n","    torch.save(state, filename)\n","    if is_best:\n","        shutil.copyfile(filename, 'neural_net.pth')"],"execution_count":13,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_rtvPIRWUsZQ","executionInfo":{"status":"ok","timestamp":1634479642641,"user_tz":-60,"elapsed":614062,"user":{"displayName":"Mitterrand Ekole","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiX0MYHQxcncfbSTaJ_vKfXwMbcZK_EEhm_DqDAtw=s64","userId":"01893583269197876978"}},"outputId":"dd091c07-f8a3-4864-b36c-8614203492d7"},"source":["criterion= nn.NLLLoss()\n","optimiser= optim.Adam(neural_net.classfier.parameters(), lr=0.001)\n","\n","cuda= torch.cuda.is_available()\n","\n","if cuda:\n","    neural_net.cuda()\n","else:\n","    neural_net.cpu()\n","    \n","epochs=200\n","print_every=5\n","save_every=50\n","steps=0\n","\n","for e in range(epochs):\n","    neural_net.train()\n","    running_loss=0\n","    accuracy_train=1\n","    \n","    for images, labels, in iter(train_loader):\n","        steps+=1\n","        inputs, labels=variable(images),variable(labels)\n","        \n","        optimiser.zero_grad()\n","        \n","        if cuda:\n","            inputs, labels=inputs.cuda(), labels.cuda()\n","            \n","        output=neural_net.forward(inputs)\n","        loss=criterion(output, labels)\n","        \n","        loss.backward()\n","        optimiser.step()\n","        \n","        running_loss+=loss.item()\n","        \n","        ps_train = torch.exp(output).data\n","        equality_train = (labels.data == ps_train.max(1)[1])\n","        accuracy_train += equality_train.type_as(torch.FloatTensor()).mean()\n","        \n","        \n","        \n","        if steps % print_every == 0:\n","            neural_net.eval()\n","            \n","            accuracy = 1\n","            valid_loss = 0\n","            \n","            for images, labels in test_loader:\n","                with torch.no_grad():\n","                    inputs = variable(images)\n","                    labels = variable(labels)\n","\n","                    if cuda:\n","                        inputs, labels = inputs.cuda(), labels.cuda()\n","\n","                    output = neural_net.forward(inputs)\n","\n","                    valid_loss += criterion(output, labels).item()\n","\n","                    ps = torch.exp(output).data\n","                    equality = (labels.data == ps.max(1)[1])\n","\n","                    accuracy += equality.type_as(torch.FloatTensor()).mean()\n","                \n","            print(\"Epoch: {}/{}.. \".format(e+1, epochs), \n","                  \"Training Loss: {:.3f}.. \".format(running_loss/print_every),\n","                  \"Validation Loss: {:.3f}..\".format(valid_loss/len(test_loader)),\n","                  \"Training Accuracy: {:.3f}\".format(accuracy_train/len(train_loader)),\n","                  \"Validation Accuracy: {:.3f}\".format(accuracy/len(test_loader)))\n","            \n","            running_loss = 0\n","            neural_net.train()\n","        if steps % save_every==0:\n","          print(\"Saving step number {}...\".format(steps))\n","          state = {'state_dict': neural_net.classifier.state_dict(),\n","                     'optimizer' : optimiser.state_dict(),\n","                     'class_to_idx':train_data.class_to_idx}\n","          model_save(state)\n","\n","            \n","     \n","            \n"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py:249: UserWarning: torch.autograd.variable(...) is deprecated, use torch.tensor(...) instead\n","  warnings.warn(\"torch.autograd.variable(...) is deprecated, use torch.tensor(...) instead\")\n","/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py:250: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  return torch.tensor(*args, **kwargs)\n"]},{"output_type":"stream","name":"stdout","text":["Epoch: 3/200..  Training Loss: 0.087..  Validation Loss: 0.321.. Training Accuracy: 0.500 Validation Accuracy: 0.500\n","Epoch: 5/200..  Training Loss: 0.152..  Validation Loss: 0.225.. Training Accuracy: 0.500 Validation Accuracy: 0.500\n","Epoch: 8/200..  Training Loss: 0.086..  Validation Loss: 0.399.. Training Accuracy: 0.500 Validation Accuracy: 0.500\n","Epoch: 10/200..  Training Loss: 0.156..  Validation Loss: 0.361.. Training Accuracy: 0.500 Validation Accuracy: 0.500\n","Epoch: 13/200..  Training Loss: 0.061..  Validation Loss: 0.338.. Training Accuracy: 0.500 Validation Accuracy: 0.500\n","Epoch: 15/200..  Training Loss: 0.175..  Validation Loss: 0.398.. Training Accuracy: 0.500 Validation Accuracy: 0.500\n","Epoch: 18/200..  Training Loss: 0.067..  Validation Loss: 0.343.. Training Accuracy: 0.500 Validation Accuracy: 0.500\n","Epoch: 20/200..  Training Loss: 0.145..  Validation Loss: 0.316.. Training Accuracy: 0.500 Validation Accuracy: 0.500\n","Epoch: 23/200..  Training Loss: 0.069..  Validation Loss: 0.408.. Training Accuracy: 0.500 Validation Accuracy: 0.500\n","Epoch: 25/200..  Training Loss: 0.159..  Validation Loss: 0.377.. Training Accuracy: 0.500 Validation Accuracy: 0.500\n","Saving step number 50...\n","Epoch: 28/200..  Training Loss: 0.076..  Validation Loss: 0.426.. Training Accuracy: 0.500 Validation Accuracy: 0.500\n","Epoch: 30/200..  Training Loss: 0.134..  Validation Loss: 0.346.. Training Accuracy: 0.500 Validation Accuracy: 0.500\n","Epoch: 33/200..  Training Loss: 0.070..  Validation Loss: 0.388.. Training Accuracy: 0.500 Validation Accuracy: 0.500\n","Epoch: 35/200..  Training Loss: 0.159..  Validation Loss: 0.337.. Training Accuracy: 0.500 Validation Accuracy: 0.500\n","Epoch: 38/200..  Training Loss: 0.100..  Validation Loss: 0.413.. Training Accuracy: 0.500 Validation Accuracy: 0.500\n","Epoch: 40/200..  Training Loss: 0.127..  Validation Loss: 0.292.. Training Accuracy: 0.500 Validation Accuracy: 0.500\n","Epoch: 43/200..  Training Loss: 0.062..  Validation Loss: 0.481.. Training Accuracy: 0.500 Validation Accuracy: 0.500\n","Epoch: 45/200..  Training Loss: 0.161..  Validation Loss: 0.410.. Training Accuracy: 0.500 Validation Accuracy: 0.500\n","Epoch: 48/200..  Training Loss: 0.066..  Validation Loss: 0.447.. Training Accuracy: 0.500 Validation Accuracy: 0.500\n","Epoch: 50/200..  Training Loss: 0.160..  Validation Loss: 0.356.. Training Accuracy: 0.500 Validation Accuracy: 0.500\n","Saving step number 100...\n","Epoch: 53/200..  Training Loss: 0.106..  Validation Loss: 0.346.. Training Accuracy: 0.500 Validation Accuracy: 0.500\n","Epoch: 55/200..  Training Loss: 0.143..  Validation Loss: 0.337.. Training Accuracy: 0.500 Validation Accuracy: 0.500\n","Epoch: 58/200..  Training Loss: 0.078..  Validation Loss: 0.327.. Training Accuracy: 0.500 Validation Accuracy: 0.500\n","Epoch: 60/200..  Training Loss: 0.136..  Validation Loss: 0.386.. Training Accuracy: 0.500 Validation Accuracy: 0.500\n","Epoch: 63/200..  Training Loss: 0.062..  Validation Loss: 0.357.. Training Accuracy: 0.500 Validation Accuracy: 0.500\n","Epoch: 65/200..  Training Loss: 0.168..  Validation Loss: 0.314.. Training Accuracy: 0.500 Validation Accuracy: 0.500\n","Epoch: 68/200..  Training Loss: 0.085..  Validation Loss: 0.321.. Training Accuracy: 0.500 Validation Accuracy: 0.500\n","Epoch: 70/200..  Training Loss: 0.175..  Validation Loss: 0.430.. Training Accuracy: 0.500 Validation Accuracy: 0.500\n","Epoch: 73/200..  Training Loss: 0.068..  Validation Loss: 0.392.. Training Accuracy: 0.500 Validation Accuracy: 0.500\n","Epoch: 75/200..  Training Loss: 0.135..  Validation Loss: 0.403.. Training Accuracy: 0.500 Validation Accuracy: 0.500\n","Saving step number 150...\n","Epoch: 78/200..  Training Loss: 0.090..  Validation Loss: 0.345.. Training Accuracy: 0.500 Validation Accuracy: 0.500\n","Epoch: 80/200..  Training Loss: 0.151..  Validation Loss: 0.402.. Training Accuracy: 0.500 Validation Accuracy: 0.500\n","Epoch: 83/200..  Training Loss: 0.044..  Validation Loss: 0.323.. Training Accuracy: 0.500 Validation Accuracy: 0.500\n","Epoch: 85/200..  Training Loss: 0.147..  Validation Loss: 0.336.. Training Accuracy: 0.500 Validation Accuracy: 0.500\n","Epoch: 88/200..  Training Loss: 0.091..  Validation Loss: 0.456.. Training Accuracy: 0.500 Validation Accuracy: 0.500\n","Epoch: 90/200..  Training Loss: 0.165..  Validation Loss: 0.385.. Training Accuracy: 0.500 Validation Accuracy: 0.500\n","Epoch: 93/200..  Training Loss: 0.084..  Validation Loss: 0.303.. Training Accuracy: 0.500 Validation Accuracy: 0.500\n","Epoch: 95/200..  Training Loss: 0.164..  Validation Loss: 0.329.. Training Accuracy: 0.500 Validation Accuracy: 0.500\n","Epoch: 98/200..  Training Loss: 0.076..  Validation Loss: 0.382.. Training Accuracy: 0.500 Validation Accuracy: 0.500\n","Epoch: 100/200..  Training Loss: 0.161..  Validation Loss: 0.436.. Training Accuracy: 0.500 Validation Accuracy: 0.500\n","Saving step number 200...\n","Epoch: 103/200..  Training Loss: 0.062..  Validation Loss: 0.383.. Training Accuracy: 0.500 Validation Accuracy: 0.500\n","Epoch: 105/200..  Training Loss: 0.133..  Validation Loss: 0.383.. Training Accuracy: 0.500 Validation Accuracy: 0.500\n","Epoch: 108/200..  Training Loss: 0.059..  Validation Loss: 0.393.. Training Accuracy: 0.500 Validation Accuracy: 0.500\n","Epoch: 110/200..  Training Loss: 0.162..  Validation Loss: 0.433.. Training Accuracy: 0.500 Validation Accuracy: 0.500\n","Epoch: 113/200..  Training Loss: 0.073..  Validation Loss: 0.327.. Training Accuracy: 0.500 Validation Accuracy: 0.500\n","Epoch: 115/200..  Training Loss: 0.147..  Validation Loss: 0.289.. Training Accuracy: 0.500 Validation Accuracy: 0.500\n","Epoch: 118/200..  Training Loss: 0.058..  Validation Loss: 0.359.. Training Accuracy: 0.500 Validation Accuracy: 0.500\n","Epoch: 120/200..  Training Loss: 0.161..  Validation Loss: 0.383.. Training Accuracy: 0.500 Validation Accuracy: 0.500\n","Epoch: 123/200..  Training Loss: 0.059..  Validation Loss: 0.374.. Training Accuracy: 0.500 Validation Accuracy: 0.500\n","Epoch: 125/200..  Training Loss: 0.148..  Validation Loss: 0.409.. Training Accuracy: 0.500 Validation Accuracy: 0.500\n","Saving step number 250...\n","Epoch: 128/200..  Training Loss: 0.083..  Validation Loss: 0.401.. Training Accuracy: 0.500 Validation Accuracy: 0.500\n","Epoch: 130/200..  Training Loss: 0.158..  Validation Loss: 0.359.. Training Accuracy: 0.500 Validation Accuracy: 0.500\n","Epoch: 133/200..  Training Loss: 0.064..  Validation Loss: 0.352.. Training Accuracy: 0.500 Validation Accuracy: 0.500\n","Epoch: 135/200..  Training Loss: 0.129..  Validation Loss: 0.382.. Training Accuracy: 0.500 Validation Accuracy: 0.500\n","Epoch: 138/200..  Training Loss: 0.083..  Validation Loss: 0.413.. Training Accuracy: 0.500 Validation Accuracy: 0.500\n","Epoch: 140/200..  Training Loss: 0.139..  Validation Loss: 0.474.. Training Accuracy: 0.500 Validation Accuracy: 0.500\n","Epoch: 143/200..  Training Loss: 0.076..  Validation Loss: 0.404.. Training Accuracy: 0.500 Validation Accuracy: 0.500\n","Epoch: 145/200..  Training Loss: 0.163..  Validation Loss: 0.400.. Training Accuracy: 0.500 Validation Accuracy: 0.500\n","Epoch: 148/200..  Training Loss: 0.053..  Validation Loss: 0.377.. Training Accuracy: 0.500 Validation Accuracy: 0.500\n","Epoch: 150/200..  Training Loss: 0.154..  Validation Loss: 0.407.. Training Accuracy: 0.500 Validation Accuracy: 0.500\n","Saving step number 300...\n","Epoch: 153/200..  Training Loss: 0.081..  Validation Loss: 0.422.. Training Accuracy: 0.500 Validation Accuracy: 0.500\n","Epoch: 155/200..  Training Loss: 0.138..  Validation Loss: 0.390.. Training Accuracy: 0.500 Validation Accuracy: 0.500\n","Epoch: 158/200..  Training Loss: 0.076..  Validation Loss: 0.468.. Training Accuracy: 0.500 Validation Accuracy: 0.500\n","Epoch: 160/200..  Training Loss: 0.137..  Validation Loss: 0.377.. Training Accuracy: 0.500 Validation Accuracy: 0.500\n","Epoch: 163/200..  Training Loss: 0.081..  Validation Loss: 0.407.. Training Accuracy: 0.500 Validation Accuracy: 0.500\n","Epoch: 165/200..  Training Loss: 0.144..  Validation Loss: 0.351.. Training Accuracy: 0.500 Validation Accuracy: 0.500\n","Epoch: 168/200..  Training Loss: 0.091..  Validation Loss: 0.475.. Training Accuracy: 0.500 Validation Accuracy: 0.500\n","Epoch: 170/200..  Training Loss: 0.153..  Validation Loss: 0.342.. Training Accuracy: 0.500 Validation Accuracy: 0.500\n","Epoch: 173/200..  Training Loss: 0.106..  Validation Loss: 0.377.. Training Accuracy: 0.500 Validation Accuracy: 0.500\n","Epoch: 175/200..  Training Loss: 0.165..  Validation Loss: 0.400.. Training Accuracy: 0.500 Validation Accuracy: 0.500\n","Saving step number 350...\n","Epoch: 178/200..  Training Loss: 0.062..  Validation Loss: 0.420.. Training Accuracy: 0.500 Validation Accuracy: 0.500\n","Epoch: 180/200..  Training Loss: 0.148..  Validation Loss: 0.376.. Training Accuracy: 0.500 Validation Accuracy: 0.500\n","Epoch: 183/200..  Training Loss: 0.074..  Validation Loss: 0.443.. Training Accuracy: 0.500 Validation Accuracy: 0.500\n","Epoch: 185/200..  Training Loss: 0.147..  Validation Loss: 0.338.. Training Accuracy: 0.500 Validation Accuracy: 0.500\n","Epoch: 188/200..  Training Loss: 0.076..  Validation Loss: 0.393.. Training Accuracy: 0.500 Validation Accuracy: 0.500\n","Epoch: 190/200..  Training Loss: 0.153..  Validation Loss: 0.286.. Training Accuracy: 0.500 Validation Accuracy: 0.500\n","Epoch: 193/200..  Training Loss: 0.096..  Validation Loss: 0.364.. Training Accuracy: 0.500 Validation Accuracy: 0.500\n","Epoch: 195/200..  Training Loss: 0.141..  Validation Loss: 0.395.. Training Accuracy: 0.500 Validation Accuracy: 0.500\n","Epoch: 198/200..  Training Loss: 0.073..  Validation Loss: 0.352.. Training Accuracy: 0.500 Validation Accuracy: 0.500\n","Epoch: 200/200..  Training Loss: 0.137..  Validation Loss: 0.538.. Training Accuracy: 0.500 Validation Accuracy: 0.500\n","Saving step number 400...\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Rf3uSY84UsZS","executionInfo":{"status":"ok","timestamp":1634479690433,"user_tz":-60,"elapsed":1621,"user":{"displayName":"Mitterrand Ekole","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiX0MYHQxcncfbSTaJ_vKfXwMbcZK_EEhm_DqDAtw=s64","userId":"01893583269197876978"}},"outputId":"408d07f5-972c-4494-9073-92149fb5d03b"},"source":["\n","\n","neural_net.eval()\n","criterion = nn.NLLLoss()\n","\n","\n","if cuda:\n","    neural_net.cuda()\n","else:\n","    neural_net.cpu()\n","    \n","accuracy = 0\n","test_loss = 0\n","\n","for images, labels in test_loader:\n","    with torch.no_grad():\n","        inputs = variable(images)\n","        labels = variable(labels)\n","\n","        if cuda:\n","            inputs, labels = inputs.cuda(), labels.cuda()\n","\n","        output = neural_net.forward(inputs)\n","\n","        test_loss += criterion(output, labels).item()\n","\n","        ps = torch.exp(output).data\n","        equality = (labels.data == ps.max(1)[1])\n","\n","        accuracy += equality.type_as(torch.FloatTensor()).mean()\n","\n","print(\"Test Loss: {:.3f}..\".format(test_loss/len(test_loader)),\n","      \"Test Accuracy: {:.3f}\".format(accuracy/len(test_loader)))\n"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py:249: UserWarning: torch.autograd.variable(...) is deprecated, use torch.tensor(...) instead\n","  warnings.warn(\"torch.autograd.variable(...) is deprecated, use torch.tensor(...) instead\")\n","/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py:250: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  return torch.tensor(*args, **kwargs)\n"]},{"output_type":"stream","name":"stdout","text":["Test Loss: 0.350.. Test Accuracy: 0.000\n"]}]},{"cell_type":"code","metadata":{"id":"GHxPpPFwUsZT"},"source":["def train_model(model, criterion, optimizer, scheduler, num_epochs=25):\n","    since = time.time()\n","\n","    best_model_wts = copy.deepcopy(model.state_dict())\n","    best_acc = 0.0\n","\n","    for epoch in range(num_epochs):\n","        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n","        print('-' * 10)\n","\n","        # Each epoch has a training and validation phase\n","        for phase in ['Train', 'Test']:\n","            if phase == 'Train':\n","                scheduler.step()\n","                model.train()  # Set model to training mode\n","            else:\n","                model.eval()   # Set model to evaluate mode\n","\n","            running_loss = 0.0\n","            running_corrects = 0\n","\n","            # Iterate over data.\n","            for images, labels in train_data:\n","                inputs = images\n","                labels = labels\n","\n","                # zero the parameter gradients\n","                optimizer.zero_grad()\n","\n","                # forward\n","                # track history if only in train\n","                with torch.set_grad_enabled(phase == 'Train'):\n","                    outputs = model(inputs)\n","                    _, preds = torch.max(outputs, 1)\n","                    loss = criterion(outputs, labels)\n","\n","                    # backward + optimize only if in training phase\n","                    if phase == 'Train':\n","                        loss.backward()\n","                        optimizer.step()\n","\n","                # statistics\n","                running_loss += loss.item() * inputs.size(0)\n","                running_corrects += torch.sum(preds == labels.data)\n","                \n","\n","            epoch_loss = running_loss / len(train_data)\n","            epoch_acc = running_corrects.double() / len(train_data)\n","\n","            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n","                phase, epoch_loss, epoch_acc))\n","\n","            # deep copy the model\n","            if phase == 'Test' and epoch_acc > best_acc:\n","                best_acc = epoch_acc\n","                best_model_wts = copy.deepcopy(model.state_dict())\n","\n","        print()\n","\n","    time_elapsed = time.time() - since\n","    print('Training complete in {:.0f}m {:.0f}s'.format(\n","        time_elapsed // 60, time_elapsed % 60))\n","    print('Best val Acc: {:4f}'.format(best_acc))\n","\n","    # load best model weights\n","    model.load_state_dict(best_model_wts)\n","    return model\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":458},"id":"mkPU7H7MUsZU","executionInfo":{"status":"error","timestamp":1634477279048,"user_tz":-60,"elapsed":695,"user":{"displayName":"Mitterrand Ekole","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiX0MYHQxcncfbSTaJ_vKfXwMbcZK_EEhm_DqDAtw=s64","userId":"01893583269197876978"}},"outputId":"980d20d4-eaa2-4ffb-ea2c-c46a50d18cb4"},"source":["#Loading the pretrained RESNET Model\n","\n","model_ft = models.resnet50(pretrained=True)\n","\n","num_ftrs = model_ft.fc.in_features\n","model_ft.fc = nn.Linear(num_ftrs, 2)\n","\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","model_ft = model_ft.to(device)\n","\n","criterion = nn.CrossEntropyLoss()\n","# Observe that all parameters are being optimized\n","optimizer_ft = optim.SGD(model_ft.parameters(), lr=0.001, momentum=0.9)\n","\n","# Decay LR by a factor of 0.1 every 7 epochs\n","exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)\n","\n","model_ft = train_model(model_ft, criterion, optimizer_ft, exp_lr_scheduler,\n","                       num_epochs=15)\n","\n","torch.save(model_ft.state_dict(), 'malaria_detector.pt')\n"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 0/14\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:134: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n","  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"]},{"output_type":"error","ename":"RuntimeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-42-f46f157d24a8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m model_ft = train_model(model_ft, criterion, optimizer_ft, exp_lr_scheduler,\n\u001b[0;32m---> 19\u001b[0;31m                        num_epochs=15)\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_ft\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'malaria_detector.pt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-41-4ad8257c2fb6>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, criterion, optimizer, scheduler, num_epochs)\u001b[0m\n\u001b[1;32m     31\u001b[0m                 \u001b[0;31m# track history if only in train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_grad_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mphase\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'Train'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m                     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m                     \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m                     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchvision/models/resnet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    247\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    248\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 249\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchvision/models/resnet.py\u001b[0m in \u001b[0;36m_forward_impl\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    230\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_forward_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m         \u001b[0;31m# See note [TorchScript super()]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 232\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    233\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    441\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 443\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    445\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    438\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[1;32m    439\u001b[0m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0;32m--> 440\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    441\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: Expected 4-dimensional input for 4-dimensional weight [64, 3, 7, 7], but got 3-dimensional input of size [3, 224, 224] instead"]}]},{"cell_type":"code","metadata":{"id":"k4elYKkfUsZW"},"source":[""],"execution_count":null,"outputs":[]}]}